{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib.request\n",
    "import time\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.getcompanyinfo.com/industry/information-technology-services/\"\n",
    "html = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "soup = bs(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "Company = []\n",
    "Link = []\n",
    "Type = []\n",
    "Industry = []\n",
    "Founded = []\n",
    "Country = []\n",
    "City = []\n",
    "State = []\n",
    "Phone = []\n",
    "Email = []\n",
    "Website = []\n",
    "Employees = []\n",
    "\n",
    "\n",
    "Companies = [x.get_text() for x in soup.find_all(title = \"Company Name\")]\n",
    "Links = [x['href'] for x in soup.find_all(title = \"Company Name\")]\n",
    "url_base = \"https://www.getcompanyinfo.com\"\n",
    "\n",
    "\n",
    "for n in range(len(Companies)):\n",
    "    html2 = urllib.request.urlopen(url_base+Links[n]).read().decode('utf-8')\n",
    "    soup2 = bs(html2)\n",
    "    Company.append(Companies[n])\n",
    "    Link.append(Links[n])\n",
    "    Type.append(soup2.find(\"label\",text=\"Company Type:\").find_next_sibling().get_text())\n",
    "    Industry.append(soup2.find(\"label\",text=\"Industry:\").find_next_sibling().get_text())\n",
    "    Founded.append(soup2.find(\"label\",text=\"Founded:\").find_next_sibling().get_text())\n",
    "    Country.append(soup2.find(\"label\",text=\"Country:\").find_next_sibling().get_text())\n",
    "    cityState = soup2.find(\"label\",text=\"Headquarter:\").find_next_sibling().get_text()\n",
    "    split = cityState.split(\",\")\n",
    "    City.append(split[0])\n",
    "    State.append(split[1])\n",
    "    Phone.append(soup2.find(\"label\",text=\"Phone:\").find_next_sibling().get_text())\n",
    "    Email.append(soup2.find(\"label\",text=\"Email:\").find_next_sibling().get_text())\n",
    "    Website.append(soup2.find(\"label\",text=\"Website:\").find_next_sibling().get_text())\n",
    "    Employees.append(soup2.find(\"h4\",text=\"Employees\").find_next_sibling().get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "Companies[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-theology",
   "metadata": {},
   "outputs": [],
   "source": [
    "Company = []\n",
    "Link = []\n",
    "Type = []\n",
    "Industry = []\n",
    "Founded = []\n",
    "Country = []\n",
    "City = []\n",
    "State = []\n",
    "Phone = []\n",
    "Email = []\n",
    "Website = []\n",
    "Employees = []\n",
    "\n",
    "Companies = [x.get_text() for x in soup.find_all(title = \"Company Name\")]\n",
    "Links = [x['href'] for x in soup.find_all(title = \"Company Name\")]\n",
    "url_base = \"https://www.getcompanyinfo.com\"\n",
    "\n",
    "\n",
    "for n in range(len(Companies)):\n",
    "    html2 = urllib.request.urlopen(url_base+Links[n]).read().decode('utf-8')\n",
    "    soup2 = bs(html2)\n",
    "    Company.append(Companies[n])\n",
    "    Link.append(Links[n])\n",
    "    try:\n",
    "        Type.append(soup2.find(\"label\",text=\"Company Type:\").find_next_sibling().get_text())\n",
    "    except:\n",
    "        Type.append(None)\n",
    "    try:\n",
    "        Industry.append(soup2.find(\"label\",text=\"Industry:\").find_next_sibling().get_text())\n",
    "    except:\n",
    "        Industry.append(None)\n",
    "    try:\n",
    "        Founded.append(soup2.find(\"label\",text=\"Founded:\").find_next_sibling().get_text())\n",
    "    except:\n",
    "        Founded.append(None)\n",
    "    try:\n",
    "        Country.append(soup2.find(\"label\",text=\"Country:\").find_next_sibling().get_text())\n",
    "    except:\n",
    "        Country.append(None)\n",
    "    try:\n",
    "        cityState = soup2.find(\"label\",text=\"Headquarter:\").find_next_sibling().get_text()\n",
    "        split = cityState.split(\",\")\n",
    "        City.append(split[0])\n",
    "        State.append(split[1])\n",
    "    except:\n",
    "        City.append(None)\n",
    "        State.append(None)\n",
    "    try:\n",
    "        Phone.append(soup2.find(\"label\",text=\"Phone:\").find_next_sibling().get_text())\n",
    "    except:\n",
    "        Phone.append(None)\n",
    "    try:\n",
    "        Email.append(soup2.find(\"label\",text=\"Email:\").find_next_sibling().get_text())\n",
    "    except:\n",
    "        Email.append(None)\n",
    "    try:\n",
    "        Website.append(soup2.find(\"label\",text=\"Website:\").find_next_sibling().get_text())\n",
    "    except:\n",
    "        Website.append(None)\n",
    "    try:\n",
    "        Employees.append(soup2.find(\"h4\",text=\"Employees\").find_next_sibling().get_text())\n",
    "    except:\n",
    "        Employees.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-editor",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Company\":Company,\n",
    "             \"Link\":Link,\n",
    "             \"Type\":Type,\n",
    "             \"Industry\":Industry,\n",
    "             \"Founded\":Founded,\n",
    "             \"Country\":Country,\n",
    "             \"City\":City,\n",
    "             \"State\":State,\n",
    "             \"Phone\":Phone,\n",
    "             \"Email\":Email,\n",
    "             \"Website\":Website,\n",
    "             \"Employees\":Employees})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-share",
   "metadata": {},
   "source": [
    "Now we need to tackle the next page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for next page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-quick",
   "metadata": {},
   "outputs": [],
   "source": [
    "Company = []\n",
    "Link = []\n",
    "Type = []\n",
    "Industry = []\n",
    "Founded = []\n",
    "Country = []\n",
    "City = []\n",
    "State = []\n",
    "Phone = []\n",
    "Email = []\n",
    "Website = []\n",
    "Employees = []\n",
    "\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({\"Company\":Company,\n",
    "             \"Link\":Link,\n",
    "             \"Type\":Type,\n",
    "             \"Industry\":Industry,\n",
    "             \"Founded\":Founded,\n",
    "             \"Country\":Country,\n",
    "             \"City\":City,\n",
    "             \"State\":State,\n",
    "             \"Phone\":Phone,\n",
    "             \"Email\":Email,\n",
    "             \"Website\":Website,\n",
    "             \"Employees\":Employees})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Companies.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-martin",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
